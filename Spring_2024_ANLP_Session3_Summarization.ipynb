{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# https://tinyurl.com/ANLPColab3Part1\n",
        "Go to \"File\" -> \"Save a Copy in Drive...\"\n",
        "This lets you create your own copy of the notebook in your Google drive, and any changes you make doesn't impact the shared notebook"
      ],
      "metadata": {
        "id": "mR2O-rQ6dWuj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c74LgE97xKra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extractive Summarization\n",
        "\n",
        "Extractive summarization involves selecting important sentences, phrases, or paragraphs directly from the source text and combining them to create a summary rather than generate a new text. The key idea is to identify and extract the most significant portions of the text.\n"
      ],
      "metadata": {
        "id": "C-lUWaphEwK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1: Custom defining *TextRank*\n",
        "\n",
        "### Let's run it step-by-step by defining the TextRank function"
      ],
      "metadata": {
        "id": "qCQEYmE9H-2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required libraries\n",
        "!pip install networkx nltk\n",
        "\n",
        "# Import libraries and download NLTK data\n",
        "import networkx as nx\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR5Klu5GIBqE",
        "outputId": "4f894404-c453-40bc-d83d-16d0cf6ace02"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define TextRank functions\n",
        "\n",
        "#Function to calculate the similarity between two sentences - uses cosine similarity metric\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    #create a set of all unique words from both sentences\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    #create vector representations for each sentence based on word frequencies\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    for w in sent1:\n",
        "        if w not in stopwords:\n",
        "            vector1[all_words.index(w)] += 1\n",
        "\n",
        "    for w in sent2:\n",
        "        if w not in stopwords:\n",
        "            vector2[all_words.index(w)] += 1\n",
        "\n",
        "    #calculate the cosine similarity between these vectors\n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "#Function to create the similarity matrix for all sentences\n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "\n",
        "    #initialize a zero matrix of size(number of sentences) x (number of sentences)\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    #fill this matrix with similarity scores between each pair of sentences\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2:\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "\n",
        "    return similarity_matrix\n",
        "\n",
        "#function that implements the TextRank algorithm\n",
        "def generate_summary(text, top_n=5):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    summarize_text = []\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_words = [word_tokenize(sent.lower()) for sent in sentences]\n",
        "\n",
        "    #create the graph and calculate similarities\n",
        "    similarity_matrix = build_similarity_matrix(sentence_words, stop_words)\n",
        "\n",
        "    sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph) #calculate the TextRank score\n",
        "\n",
        "    #sort the sentences based on their scores and select the top N sentences\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    #Reorder selected sentences and generate summary\n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(ranked_sentences[i][1])\n",
        "\n",
        "    return \" \".join(summarize_text)"
      ],
      "metadata": {
        "id": "VGy3pmcoIHVc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment with a text\n",
        "text = \"\"\"\n",
        "Deep learning (also known as deep structured learning) is part of a\n",
        "broader family of machine learning methods based on artificial neural networks with\n",
        "representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
        "recurrent neural networks and convolutional neural networks have been applied to\n",
        "fields including computer vision, speech recognition, natural language processing,\n",
        "machine translation, bioinformatics, drug design, medical image analysis, material\n",
        "inspection and board game programs, where they have produced results comparable to\n",
        "and in some cases surpassing human expert performance. Artificial neural networks\n",
        "(ANNs) were inspired by information processing and distributed communication nodes\n",
        "in biological systems. ANNs have various differences from biological brains. Specifically,\n",
        "neural networks tend to be static and symbolic, while the biological brain of most living organisms\n",
        "is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\n",
        "layers in the network. Early work showed that a linear perceptron cannot be a universal classifier,\n",
        "but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
        "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size,\n",
        "which permits practical application and optimized implementation, while retaining theoretical universality\n",
        "under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely\n",
        "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability,\n",
        "whence the structured part.\n",
        "\n",
        "\"\"\"\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"Original text length:\")\n",
        "print(len(text))\n",
        "\n",
        "\n",
        "print(\"\\nGenerating summary...\")\n",
        "print(\"\\n\")\n",
        "\n",
        "new_summary = generate_summary(text, top_n=1)\n",
        "print(\"Summary of provided text:\")\n",
        "print(new_summary)\n",
        "print(\"Summary text length:\")\n",
        "print(len(new_summary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkTso0W_IIYl",
        "outputId": "06b786fa-6332-478f-9118-b45783163421"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "Deep learning (also known as deep structured learning) is part of a\n",
            "broader family of machine learning methods based on artificial neural networks with\n",
            "representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
            "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
            "recurrent neural networks and convolutional neural networks have been applied to\n",
            "fields including computer vision, speech recognition, natural language processing,\n",
            "machine translation, bioinformatics, drug design, medical image analysis, material\n",
            "inspection and board game programs, where they have produced results comparable to\n",
            "and in some cases surpassing human expert performance. Artificial neural networks\n",
            "(ANNs) were inspired by information processing and distributed communication nodes\n",
            "in biological systems. ANNs have various differences from biological brains. Specifically,\n",
            "neural networks tend to be static and symbolic, while the biological brain of most living organisms\n",
            "is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\n",
            "layers in the network. Early work showed that a linear perceptron cannot be a universal classifier,\n",
            "but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
            "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size,\n",
            "which permits practical application and optimized implementation, while retaining theoretical universality\n",
            "under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely\n",
            "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability,\n",
            "whence the structured part.\n",
            "\n",
            "\n",
            "Original text length:\n",
            "1811\n",
            "\n",
            "Generating summary...\n",
            "\n",
            "\n",
            "Summary of provided text:\n",
            "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
            "recurrent neural networks and convolutional neural networks have been applied to\n",
            "fields including computer vision, speech recognition, natural language processing,\n",
            "machine translation, bioinformatics, drug design, medical image analysis, material\n",
            "inspection and board game programs, where they have produced results comparable to\n",
            "and in some cases surpassing human expert performance.\n",
            "Summary text length:\n",
            "493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying a different top-n\n",
        "print(\"Original text:\")\n",
        "print(text)\n",
        "print(\"Original text length:\")\n",
        "print(len(text))\n",
        "\n",
        "\n",
        "print(\"\\nGenerating summary...\")\n",
        "print(\"\\n\")\n",
        "\n",
        "new_summary = generate_summary(text, top_n=2)\n",
        "print(\"Summary of provided text:\")\n",
        "print(new_summary)\n",
        "print(\"Summary text length:\")\n",
        "print(len(new_summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnXSkIMqOmKJ",
        "outputId": "68628c17-7152-4d7e-c0b3-427882d0740b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "Deep learning (also known as deep structured learning) is part of a\n",
            "broader family of machine learning methods based on artificial neural networks with\n",
            "representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
            "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
            "recurrent neural networks and convolutional neural networks have been applied to\n",
            "fields including computer vision, speech recognition, natural language processing,\n",
            "machine translation, bioinformatics, drug design, medical image analysis, material\n",
            "inspection and board game programs, where they have produced results comparable to\n",
            "and in some cases surpassing human expert performance. Artificial neural networks\n",
            "(ANNs) were inspired by information processing and distributed communication nodes\n",
            "in biological systems. ANNs have various differences from biological brains. Specifically,\n",
            "neural networks tend to be static and symbolic, while the biological brain of most living organisms\n",
            "is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\n",
            "layers in the network. Early work showed that a linear perceptron cannot be a universal classifier,\n",
            "but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
            "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size,\n",
            "which permits practical application and optimized implementation, while retaining theoretical universality\n",
            "under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely\n",
            "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability,\n",
            "whence the structured part.\n",
            "\n",
            "\n",
            "Original text length:\n",
            "1811\n",
            "\n",
            "Generating summary...\n",
            "\n",
            "\n",
            "Summary of provided text:\n",
            "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
            "recurrent neural networks and convolutional neural networks have been applied to\n",
            "fields including computer vision, speech recognition, natural language processing,\n",
            "machine translation, bioinformatics, drug design, medical image analysis, material\n",
            "inspection and board game programs, where they have produced results comparable to\n",
            "and in some cases surpassing human expert performance. In deep learning the layers are also permitted to be heterogeneous and to deviate widely\n",
            "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability,\n",
            "whence the structured part.\n",
            "Summary text length:\n",
            "723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_summary = generate_summary(text, top_n=1)\n",
        "print(\"Summary of provided text:\")\n",
        "print(new_summary)\n",
        "print(\"Summary text length:\")\n",
        "print(len(new_summary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1NSG4VPO2KE",
        "outputId": "40b02632-ce49-4aa7-f11d-e4cdbd8971ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of provided text:\n",
            "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
            "recurrent neural networks and convolutional neural networks have been applied to\n",
            "fields including computer vision, speech recognition, natural language processing,\n",
            "machine translation, bioinformatics, drug design, medical image analysis, material\n",
            "inspection and board game programs, where they have produced results comparable to\n",
            "and in some cases surpassing human expert performance.\n",
            "Summary text length:\n",
            "493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Using Python packages\n",
        "\n",
        "### Now, let's run it using pre-defined functions in the Spacy and PyTextRank python libraries"
      ],
      "metadata": {
        "id": "X_r8Qun1IL69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "!pip install pytextrank #a spaCy extension that effectively implements the TextRank algorithm\n",
        "\n",
        "import spacy\n",
        "import pytextrank\n",
        "#load spacy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "7myGJkC0VRgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044339bf-ea6f-4c55-dfa4-96291054deb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytextrank\n",
            "  Downloading pytextrank-3.3.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: GitPython>=3.1 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (3.1.43)\n",
            "Requirement already satisfied: graphviz>=0.13 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (0.20.3)\n",
            "Collecting icecream>=2.1 (from pytextrank)\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (3.4.2)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (1.13.1)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.10/dist-packages (from pytextrank) (3.7.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=3.1->pytextrank) (4.0.11)\n",
            "Collecting colorama>=0.3.9 (from icecream>=2.1->pytextrank)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting executing>=0.3.1 (from icecream>=2.1->pytextrank)\n",
            "  Downloading executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream>=2.1->pytextrank)\n",
            "  Downloading asttokens-2.4.1-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (3.8.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from networkx[default]>=2.6->pytextrank) (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.0->pytextrank) (3.4.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1->pytextrank) (5.0.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0->pytextrank) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->networkx[default]>=2.6->pytextrank) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->networkx[default]>=2.6->pytextrank) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->pytextrank) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0->pytextrank) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.0->pytextrank) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->pytextrank) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->pytextrank) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->pytextrank) (13.9.3)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->pytextrank) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->pytextrank) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.0->pytextrank) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0->pytextrank) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->pytextrank) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0->pytextrank) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->pytextrank) (0.1.2)\n",
            "Downloading pytextrank-3.3.0-py3-none-any.whl (26 kB)\n",
            "Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream, pytextrank\n",
            "Successfully installed asttokens-2.4.1 colorama-0.4.6 executing-2.1.0 icecream-2.1.3 pytextrank-3.3.0\n",
            "/usr/local/lib/python3.10/dist-packages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xe1jp0j9Rrcb"
      },
      "outputs": [],
      "source": [
        "#Load Text\n",
        "\n",
        "text = '''Deep learning (also known as deep structured learning) is part of a\n",
        "broader family of machine learning methods based on artificial neural networks with\n",
        "representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
        "Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning,\n",
        "recurrent neural networks and convolutional neural networks have been applied to\n",
        "fields including computer vision, speech recognition, natural language processing,\n",
        "machine translation, bioinformatics, drug design, medical image analysis, material\n",
        "inspection and board game programs, where they have produced results comparable to\n",
        "and in some cases surpassing human expert performance. Artificial neural networks\n",
        "(ANNs) were inspired by information processing and distributed communication nodes\n",
        "in biological systems. ANNs have various differences from biological brains. Specifically,\n",
        "neural networks tend to be static and symbolic, while the biological brain of most living organisms\n",
        "is dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\n",
        "layers in the network. Early work showed that a linear perceptron cannot be a universal classifier,\n",
        "but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\n",
        "Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size,\n",
        "which permits practical application and optimized implementation, while retaining theoretical universality\n",
        "under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely\n",
        "from biologically informed connectionist models, for the sake of efficiency, trainability and understandability,\n",
        "whence the structured part.'''\n",
        "\n",
        "text = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loads the model with the TextRank summarization pipeline\n",
        "nlp.add_pipe(\"textrank\")"
      ],
      "metadata": {
        "id": "JNAkbj1A4es-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c2ac4c6-039e-403d-f682-5ea9b07b8e71"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pytextrank.base.BaseTextRankFactory at 0x7d4371531ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text summarization\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print('Original Document Size:',len(text)) #number of characters\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "print(\"\\nGenerating summary...\")\n",
        "print(\"\\n\")\n",
        "\n",
        "doc = nlp(text)\n",
        "summary = ''\n",
        "summarySize = 0\n",
        "\n",
        "#Limit summary to 2 phrases and 2 sentences\n",
        "for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
        "    summary = summary + \" \" +str(sent)\n",
        "    summarySize += len(sent) # counts characters in the selected sentences\n",
        "\n",
        "print(\"Summary :\", summary)\n",
        "print(\"\\n\")\n",
        "print(\"Summary Size :\", summarySize)\n",
        "\n",
        "print(\"Summary length:\")\n",
        "print(len(summary)) #might differ from summary size due to additional spaces"
      ],
      "metadata": {
        "id": "EP6VUMKPVc7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e88e2842-f039-469b-950d-1347c790f438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "\n",
            "\n",
            "Original Document Size: 2\n",
            "\n",
            "\n",
            "\n",
            "Generating summary...\n",
            "\n",
            "\n",
            "Summary :  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Summary Size : 1\n",
            "Summary length:\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jL5gUPI6BGZA"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}